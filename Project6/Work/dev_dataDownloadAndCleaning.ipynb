{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Projet 6: CatÃ©gorisez automatiquement des questions](https://openclassrooms.com/fr/projects/categorisez-automatiquement-des-questions)\n",
    "(parcours data: [here](https://openclassrooms.com/paths/63-data-scientist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data exporter (StackExchange): [here](https://data.stackexchange.com/stackoverflow/query/new).  \n",
    "My minimal SQL query:\n",
    "```\n",
    "SELECT\n",
    "   Id,Body,Title,Tags\n",
    "FROM\n",
    "   Posts\n",
    "WHERE\n",
    "   Id > 800000 and Id < 1000000 and Body<>'' and Title<>'' and Tags <>''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "- [tutorial on kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words) (I did this basically on my own, but look at the features with scikit-learn section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.path.expanduser('~/')\n",
    "HOST = os.uname()[1]\n",
    "if HOST == 'Arthurs-MacBook-Pro.local':\n",
    "    os.chdir(HOME+'Documents/GitHub/OCDataSciencePath/Project6/Work/')    # @home\n",
    "else:\n",
    "    raise ValueError('unknown host: {}'.format(HOST))\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info and uses for [BeautifulSoup](https://pypi.org/project/beautifulsoup4/): [here](https://www.pythonforbeginners.com/beautifulsoup/beautifulsoup-4-python),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # conda install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info and uses for [NLTK](https://pypi.org/project/nltk/): [here](http://www.nltk.org/book/),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pkg.helper import basicHTMLTextCleaner, basicTagTextCleaner, isValidData # needs correct working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HOST == 'Arthurs-MacBook-Pro.local':\n",
    "    pathToDataDir = HOME+'Documents/Dropbox/Transit/OCDataScienceData/Project6/Data/'    # @home\n",
    "    pathToIntellDir = HOME+'Documents/Dropbox/Transit/OCDataScienceData/Project6/Intelligence/'    # @home\n",
    "else:\n",
    "    raise ValueError('unknown host: {}'.format(HOST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174696, 3)\n"
     ]
    }
   ],
   "source": [
    "# filename = 'QueryResults_10k.csv'\n",
    "# df = pd.read_csv(pathToDataDir+filename,index_col='Id')\n",
    "\n",
    "first = True\n",
    "for i in range(1,8):\n",
    "# for i in range(1,2):\n",
    "    filename = 'QueryResults-{}.csv'.format(i)\n",
    "    df_ = pd.read_csv(pathToDataDir+filename,index_col='Id')\n",
    "    if first:\n",
    "        df = df_.copy()\n",
    "        first = False\n",
    "    else:\n",
    "        df = df.append(df_)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = df.shape\n",
    "k = .8\n",
    "idx_train = np.random.choice((True,False),n,p=(k,1-k))\n",
    "df_train,df_test = df.iloc[idx_train,:].copy(),df.iloc[~idx_train,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140106, 3) (34590, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body</th>\n",
       "      <th>Title</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>696573</th>\n",
       "      <td>&lt;p&gt;I have a function getSlope which takes as p...</td>\n",
       "      <td>Unexpected loss of precision when dividing dou...</td>\n",
       "      <td>&lt;c++&gt;&lt;double&gt;&lt;precision&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46354</th>\n",
       "      <td>&lt;p&gt;I'm trying to perform a SQL query through a...</td>\n",
       "      <td>\"Invalid column name\" error on SQL statement f...</td>\n",
       "      <td>&lt;sql&gt;&lt;sql-server&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607250</th>\n",
       "      <td>&lt;p&gt;I have a generic type that is parameterized...</td>\n",
       "      <td>How do I get the type object of a genericized ...</td>\n",
       "      <td>&lt;java&gt;&lt;generics&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902017</th>\n",
       "      <td>&lt;p&gt;I saw a program(RAPGET) which downloads a f...</td>\n",
       "      <td>Data from web-page to a text box in application?</td>\n",
       "      <td>&lt;vb6&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399538</th>\n",
       "      <td>&lt;p&gt;I'm building a C++ application and need to ...</td>\n",
       "      <td>How do I link PDCurses to a C++ application on...</td>\n",
       "      <td>&lt;c++&gt;&lt;windows&gt;&lt;linker&gt;&lt;ncurses&gt;&lt;pdcurses&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Body  \\\n",
       "Id                                                          \n",
       "696573  <p>I have a function getSlope which takes as p...   \n",
       "46354   <p>I'm trying to perform a SQL query through a...   \n",
       "607250  <p>I have a generic type that is parameterized...   \n",
       "902017  <p>I saw a program(RAPGET) which downloads a f...   \n",
       "399538  <p>I'm building a C++ application and need to ...   \n",
       "\n",
       "                                                    Title  \\\n",
       "Id                                                          \n",
       "696573  Unexpected loss of precision when dividing dou...   \n",
       "46354   \"Invalid column name\" error on SQL statement f...   \n",
       "607250  How do I get the type object of a genericized ...   \n",
       "902017   Data from web-page to a text box in application?   \n",
       "399538  How do I link PDCurses to a C++ application on...   \n",
       "\n",
       "                                             Tags  \n",
       "Id                                                 \n",
       "696573                   <c++><double><precision>  \n",
       "46354                           <sql><sql-server>  \n",
       "607250                           <java><generics>  \n",
       "902017                                      <vb6>  \n",
       "399538  <c++><windows><linker><ncurses><pdcurses>  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape,df_test.shape)\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyCleaner(df,tokenizer,stopwords,stemer):\n",
    "    '''\n",
    "    applies the cleaner to a pd.DataFrame. See called functions for help.\n",
    "    '''\n",
    "    for c in ('Body','Title'):\n",
    "        df[c+'_clean'] = df[c].apply(lambda x: basicHTMLTextCleaner(x,tokenizer,stopwords,stemer))\n",
    "    for c in ('Tags',):\n",
    "        df[c+'_clean'] = df[c].apply(lambda x: basicTagTextCleaner(x))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the potential tokens (to be used with RegexpTokenizer(gaps=False))\n",
    "rex = ''\n",
    "rex += '\\.?'          # token can start by a dot\n",
    "rex += '[a-zA-Z]+'    # then a word in letters\n",
    "rex += '[0-9#+]*'     # then potentially numbers or some symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(rex,gaps=False)\n",
    "stemer = nltk.PorterStemmer()\n",
    "\n",
    "tic = time.time()\n",
    "df_train = applyCleaner(df_train,tokenizer,stopwords,stemer)\n",
    "df_test = applyCleaner(df_test,tokenizer,stopwords,stemer)\n",
    "print('done: took {}s'.format(np.round(time.time()-tic,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transformers for later\n",
    "pickle.dump(stopwords, open(pathToIntellDir+'stopwords.pkl','wb'))\n",
    "pickle.dump(tokenizer, open(pathToIntellDir+'tokenizer.pkl','wb'))\n",
    "pickle.dump(stemer, open(pathToIntellDir+'stemer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "Resources\n",
    "- [scikit tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) (also used in this [kaggle example](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Input texts:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count occurences (beware to fit the dictionary only on train data!)\n",
    "# NB: deal with tags separately, as they basically need one-hot encoding\n",
    "max_df = .3 # should be possible to push up to .1 given the low frequencies of the tags...\n",
    "min_df = .01\n",
    "\n",
    "V_body = 500 # 100 # 20 # 500\n",
    "count_body = CountVectorizer(analyzer = 'word',   # entities to be counted\n",
    "                             strip_accents = None,\n",
    "                             preprocessor = None, # because we did it ourselves\n",
    "                             lowercase = False,   # because we did it ourselves\n",
    "                             stop_words = None,   # because we did it ourselves\n",
    "                             tokenizer = None,    # because we did it ourselves\n",
    "                             token_pattern = rex, # must rewrite the tokens!\n",
    "                             max_df = max_df,     # perform additional stopword trimming \n",
    "                             min_df = min_df,     # drop too rare terms\n",
    "                             max_features = V_body) \n",
    "\n",
    "count_body.fit(df_train['Body_clean'])\n",
    "\n",
    "V_title = 100 # 20 # 5 # 100\n",
    "count_title = CountVectorizer(analyzer = 'word',   # entities to be counted\n",
    "                              strip_accents = None,\n",
    "                              preprocessor = None, # because we did it ourselves\n",
    "                              lowercase = False,   # because we did it ourselves\n",
    "                              stop_words = None,   # because we did it ourselves\n",
    "                              tokenizer = None,    # because we did it ourselves\n",
    "                              token_pattern = rex, # must rewrite the tokens!\n",
    "                              max_df = max_df,     # perform additional stopword trimming \n",
    "                              min_df = min_df,     # drop too rare terms\n",
    "                              max_features = V_title) \n",
    "\n",
    "count_title.fit(df_train['Title_clean'])\n",
    "\n",
    "# transform to vectors using the fitted dictionaries\n",
    "body_train = count_body.transform(df_train['Body_clean'])\n",
    "title_train = count_title.transform(df_train['Title_clean'])\n",
    "\n",
    "body_test = count_body.transform(df_test['Body_clean'])\n",
    "title_test = count_title.transform(df_test['Title_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nvocab for body (length {}): {}'.format(len(count_body.vocabulary_),count_body.vocabulary_.keys()))\n",
    "print('\\nvocab for title (length {}): {}'.format(len(count_title.vocabulary_),count_title.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use TF-IDF (beware to fit the IDF with train data!)\n",
    "tfidf_body = TfidfTransformer(use_idf=True).fit(body_train)\n",
    "tfidf_title = TfidfTransformer(use_idf=True).fit(title_train)\n",
    "\n",
    "# transform using the fitted IDF\n",
    "body2_train = tfidf_body.transform(body_train)\n",
    "title2_train = tfidf_title.transform(title_train)\n",
    "\n",
    "body2_test = tfidf_body.transform(body_test)\n",
    "title2_test = tfidf_title.transform(title_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transformers for later use\n",
    "pickle.dump(count_body, open(pathToIntellDir+'count_body.pkl','wb'))\n",
    "pickle.dump(count_title, open(pathToIntellDir+'count_title.pkl','wb'))\n",
    "\n",
    "pickle.dump(tfidf_body, open(pathToIntellDir+'tfidf_body.pkl','wb'))\n",
    "pickle.dump(tfidf_title, open(pathToIntellDir+'tfidf_title.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body2_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title2_train[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Target tags:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis of the tags (NB: here we only 'multiplly'-one-hot encode: there is no need to compute TF-IDF!)\n",
    "\n",
    "# count occurences\n",
    "min_df_tag = .02 # we will only deal with not too rare tags\n",
    "\n",
    "count_tag = CountVectorizer(analyzer = \"word\",   # entities to be counted\n",
    "                            strip_accents = None,\n",
    "                            preprocessor = None, # because we did it ourselves\n",
    "                            lowercase = False,   # because we did it ourselves\n",
    "                            stop_words = None,   # because we did it ourselves\n",
    "                            tokenizer = None,    # because we did it ourselves\n",
    "                            token_pattern = '<[^[<>]*>', # specific token for these tags\n",
    "                            min_df = min_df_tag) \n",
    "\n",
    "count_tag.fit(df_train['Tags_clean'])\n",
    "\n",
    "# transform\n",
    "tag_train = count_tag.transform(df_train['Tags_clean'])\n",
    "tag_test = count_tag.transform(df_test['Tags_clean'])\n",
    "\n",
    "# plot number of questions per tag\n",
    "tagCount = pd.DataFrame(index=['count'])\n",
    "tagCount['all questions'] = tag_train.shape[0]\n",
    "for t,i in count_tag.vocabulary_.items():\n",
    "    tagCount[t] = tag_train[:,i].sum()\n",
    "tagCount = tagCount.transpose()\n",
    "tagCount = tagCount.sort_values(by='count',ascending=True)\n",
    "tagCount.plot(kind='barh',figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nvocab for tags: {}'.format(count_tag.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save transofrmer for later use\n",
    "pickle.dump(count_tag, open(pathToIntellDir+'count_tag.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_train[0,:].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Merge together:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = hstack((body2_train,title2_train)) # need hstack because we work on sparse matrices...\n",
    "x_test = hstack((body2_test,title2_test))\n",
    "\n",
    "y_train = tag_train\n",
    "y_test = tag_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handle cases when some rows do not have selected words/tags...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE HOW TO AVOID THAT: go back to full matrices for subscripting, export...\n",
    "x_train = x_train.toarray()\n",
    "x_test = x_test.toarray()\n",
    "\n",
    "y_train = y_train.toarray()\n",
    "y_test = y_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isValid_train = isValidData(x_train,y_train,V_body)\n",
    "isValid_test = isValidData(x_test,y_test,V_body)\n",
    "\n",
    "print(np.array(isValid_train).mean())\n",
    "print(np.array(isValid_test).mean())\n",
    "\n",
    "x_train,y_train = x_train[isValid_train,:],y_train[isValid_train,:]\n",
    "x_test,y_test = x_test[isValid_test,:],y_test[isValid_test,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name,ext = os.path.splitext(filename)\n",
    "\n",
    "#c = ('Body_clean','Title_clean','Tags_clean')\n",
    "#df.loc[:,c].to_csv(os.path.join(pathToDataDir,name+'_clean'+ext),index=True)\n",
    "\n",
    "np.savez(os.path.join(pathToDataDir,name+'_forLearning'), *[x_train,\n",
    "                                                            x_test,\n",
    "                                                            y_train,\n",
    "                                                            y_test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
